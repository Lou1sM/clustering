{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from fastai import datasets,layers\n",
    "from fastai.vision import ImageList\n",
    "from fastai.basic_train import Learner\n",
    "from fastai.basic_data import DataBunch\n",
    "from fastai.callback import Callback\n",
    "from ipdb import set_trace\n",
    "from torch.utils import data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from functools import partial\n",
    "import torchvision.datasets as tdatasets\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import os\n",
    "import psutil\n",
    "from shutil import rmtree\n",
    "import math\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import hdbscan\n",
    "import umap.umap_ as umap\n",
    "\n",
    "%load_ext line_profiler\n",
    "utils.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NZ=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reload():\n",
    "    import importlib, utils\n",
    "    importlib.reload(utils)\n",
    "mnist_ds = utils.get_mnist_dset(x_only=True)\n",
    "mnist_train = utils.get_mnist_dset(x_only=False)\n",
    "type(mnist_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(t,dim):\n",
    "#     x=t+torch.logsumexp(t,dim=0)\n",
    "    x=t/t.std(dim=dim,keepdim=True)\n",
    "    x = F.softmax(x,dim=dim)\n",
    "    y = (x+1e-5).log()\n",
    "    assert t.shape == x.shape and t.shape== y.shape\n",
    "    ent = (-x*y).sum(dim=dim)\n",
    "    return ent\n",
    "a = torch.randn(3,6,8)\n",
    "entropy(a,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc,dec,lin = utils.get_enc_dec('cuda',latent_size=NZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exemplar():\n",
    "    def __init__(self,tensor): self.tensor,self.count,self.age=tensor,0,0\n",
    "        \n",
    "class Concepts():\n",
    "    def __init__(self,dim,opt,thresh,num_starting_concepts=10): \n",
    "        self.dim,self.opt,self.thresh=dim,opt,thresh\n",
    "        self.exemplars = []; params = []\n",
    "        self.dist_func = nn.MSELoss()\n",
    "        self.exemplars = [Exemplar(normalize(torch.randn(self.dim,device='cuda'))) for _ in range(num_starting_concepts)]\n",
    "        params = [ex.tensor for ex in self.exemplars]\n",
    "\n",
    "        assert len(self) == num_starting_concepts == len(params)\n",
    "        self.opt.param_groups = self.opt.param_groups[:2]\n",
    "        self.opt.add_param_group({'params': params, 'lr':1e-5})\n",
    "        \n",
    "    def add_exemplar(self,tensor):\n",
    "        new_exemplar = Exemplar(tensor)\n",
    "        self.exemplars.append(new_exemplar)\n",
    "        return new_exemplar\n",
    " \n",
    "    @property\n",
    "    def exemplars_as_tensor(self): return torch.stack([e.tensor for e in self.exemplars])\n",
    "    def __len__(self): return len(self.exemplars)\n",
    "        \n",
    "    def interpret_wo_grad(self,inp_,eps):\n",
    "        for e in self.exemplars:\n",
    "            e.count+=1\n",
    "            e.age+=1\n",
    "            if e.count - math.log(e.age) > 20: \n",
    "                self.exemplars.remove(e)\n",
    "        try: dist,idx = torch.min((self.exemplars_as_tensor-inp_).norm(dim=-1),0)\n",
    "        except RuntimeError: pass # No exemplars defined yet\n",
    "        if len(self) == 0 or dist > self.thresh:\n",
    "            best_match = self.add_exemplar(inp_.data.clone().squeeze(0))\n",
    "            dist = torch.tensor(0.,device='cuda')\n",
    "        else:\n",
    "            best_match = self.exemplars[idx]\n",
    "            best_match.count = 0\n",
    "            best_match.tensor.lerp_(inp_.data.clone().squeeze(0), eps/(best_match.age+1))\n",
    "        #best_match.tensor /= best_match.tensor.norm()\n",
    "        #for e in self.exemplars: e.tensor.lerp_(best_match.tensor,-eps/(e.age+1))\n",
    "#         if len(self) > 4 and self.thresh < 10: self.thresh *= 1.05\n",
    "#         elif len(self) < 4: self.thresh *= 0.95\n",
    "        return best_match, dist \n",
    "    \n",
    "    \n",
    "class ConceptLearner():\n",
    "    def __init__(self,enc,dec,lin,dataset,opt,loss_func,discrim=None): \n",
    "        self.enc,self.dec,self.lin,self.dataset,self.loss_func,self.ce_loss_func = enc,dec,lin,dataset,loss_func,nn.CrossEntropyLoss()\n",
    "        self.opt = opt(params = [{'params':enc.parameters()},{'params':dec.parameters()}])\n",
    "        self.discrim = discrim if discrim else utils.NonsenseDiscriminator().to('cuda')\n",
    "        self.discrim_opt = opt(params=[{'params': self.discrim.parameters()}])\n",
    "        try: rmtree('runs')\n",
    "        except: pass\n",
    "        self.writer = SummaryWriter()\n",
    "        self.init_concepts(1.)\n",
    "        \n",
    "    def init_concepts(self,thresh): self.concepts = Concepts(NZ,opt=self.opt,thresh=thresh,num_starting_concepts=0)\n",
    "    def test_shapes(self): test_shapes(self.ae)\n",
    "    def show_one_concept(self,idx=0): \n",
    "        plt.imshow(self.dec((self.concepts.exemplars[idx].tensor)[None,:,None,None]).squeeze())\n",
    "        plt.show()\n",
    "        \n",
    "    def train_ae(self,epochs,bs,gan_lmbda,nonsense_lmbda,ohe_lmbda_max):\n",
    "        dl = data.DataLoader(self.dataset,batch_sampler=data.BatchSampler(data.SequentialSampler(self.dataset),bs,drop_last=True),pin_memory=False)        \n",
    "        for param in self.enc.parameters(): param.requires_grad = True\n",
    "        for param in self.dec.parameters(): param.requires_grad = True    \n",
    "        purities = [torch.tensor(0.) for i in range(4)]\n",
    "        counts = [torch.tensor(0.) for i in range(4)]\n",
    "        def purity_thresh(epoch,i): return 1-0.8*((epoch+i/len(dl))/epochs)\n",
    "        thresh = 1.\n",
    "        self.lin.weight.data -= self.lin.weight.mean(dim=-1)[:,None]\n",
    "        self.lin.bias.data.zero_()\n",
    "#         assert torch.allclose(self.lin.weight.mean(dim=-1),torch.zeros(NZ,device='cuda'),rtol=1e-2,atol=1e-3)\n",
    "        for epoch in range(epochs):\n",
    "            print(enc[1][3][0].weight.shape)\n",
    "            ohe_lmbda=(epoch/epochs)*ohe_lmbda_max\n",
    "            for i, xb in enumerate(dl): \n",
    "                latent = self.enc(xb)\n",
    "                total_latents = latent.detach().cpu() if i==0 else torch.cat([total_latents,latent.detach().cpu()],dim=0)\n",
    "#                 print(len(total_latents))\n",
    "#                 latent = latent/latent.norm(dim=1,keepdim=True)\n",
    "                preds = self.lin(latent[:,:,0,0])\n",
    "#                 ohe_loss = Categorical(probs=latent[:,:,0,0]).entropy().mean()\n",
    "                if ohe_lmbda_max > 0:\n",
    "                    target_category, clatent = utils.oheify(preds)\n",
    "                    ohe_loss = self.ce_loss_func(preds,target_category)\n",
    "                else: \n",
    "                    clatent = latent\n",
    "                    target_category = torch.argmax(preds,dim=1)\n",
    "                    ohe_loss = torch.tensor(0.).cuda()\n",
    "                rloss = self.loss_func(self.dec(latent),xb).mean(dim=[1,2,3])\n",
    "                rpredloss = self.loss_func(self.dec(clatent),xb).mean(dim=[1,2,3])\n",
    "                entropy_loss = entropy(preds,dim=1)\n",
    "                easys = entropy_loss < 0.015\n",
    "                if epoch>4:\n",
    "                    if easys.sum() > 1: print(easys.sum().item())\n",
    "                    centroid_dists = (centroids-latent[:,None,:,0,0]).norm(dim=-1)\n",
    "                    first,second = torch.topk(cehntroid_dists,2,dim=-1)\n",
    "                    set_trace()\n",
    "                    \n",
    "                loss = utils.safemean(rloss[~easys]) + utils.safemean((rpredloss+ohe_loss)[easys]) + 1*latent[:,:,0,0].norm(dim=-1).mean()+ohe_lmbda*ohe_loss\n",
    "#                 try:purities = [torch.lerp(purity,torch.sum(rloss[target_category==i].detach().cpu()),0.1) for i,purity in enumerate(purities)]\n",
    "#                 except: set_trace()\n",
    "                counts = [torch.lerp(count,torch.sum(target_category.detach().cpu()==i).float(),0.5) for i,count in enumerate(counts)]\n",
    "                loss.backward(retain_graph=True);\n",
    "                self.opt.step(); self.opt.zero_grad(); self.discrim_opt.zero_grad() \n",
    "                self.writer.add_scalar('ae/loss',loss,i)\n",
    "#             self.writer.add_images('Test_batch', pred,epoch)\n",
    "            print(loss.mean().item(),entropy_loss,counts)\n",
    "            if epoch>3 or True:\n",
    "                umapped_latents = umap.UMAP(random_state=42).fit_transform(total_latents.squeeze())\n",
    "                hdbscan_labels = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=500).fit_predict(umapped_latents)\n",
    "                ohe_labels = torch.tensor(hdbscan_labels[:,None]) == torch.arange(max(hdbscan_labels)+1)\n",
    "                d = utils.Dataholder(self.dataset.data,torch.tensor(hdbscan_labels,device='cuda'))\n",
    "                self.labeled_ds = utils.TransformDataset(d,[utils.to_float_tensor,utils.add_colour_dimension],x_only=False,device='cuda')\n",
    "                self.lin1 = nn.Linear(NZ,max(hdbscan_labels)).to('cuda')\n",
    "                self.c_loss_func=nn.CrossEntropyLoss(reduction='none')\n",
    "                self.train_epoch_labels(bs)\n",
    "#                 centroids = torch.stack([total_latents[hdbscan_labels==i,:,0,0].mean(axis=0) for i in range(max(hdbscan_labels)+1)]).cuda()\n",
    "            \n",
    "            if epoch>2 and False:\n",
    "                set_trace()\n",
    "                to_remove = [count for count in counts if count<0.01]\n",
    "                if len(to_remove) > 0: print('removing', to_remove)\n",
    "                for item in to_remove: \n",
    "                    utils.prune(enc,dec,lin,counts.index(item))\n",
    "                    del purities[counts.index(item)]\n",
    "                    counts.remove(item)\n",
    "                if len(to_remove) == 0: thresh*=0.95\n",
    "                to_separate = [p for p in purities if p>thresh]\n",
    "                if len(to_separate)>0: print('separating', to_separate)\n",
    "                for item in to_separate:\n",
    "                    idx = purities.index(item)\n",
    "                    utils.duplicate_dim(enc,dec,lin,idx)\n",
    "                    purities[idx] = torch.tensor(0.)\n",
    "                    purities += [torch.tensor(0.)]\n",
    "                    counts += counts[idx:idx+1]\n",
    "                    \n",
    "    def train_epoch_labels(self,bs):\n",
    "        dl = data.DataLoader(self.labeled_ds,batch_sampler=data.BatchSampler(data.SequentialSampler(self.dataset),bs,drop_last=True),pin_memory=False)        \n",
    "        for i, (xb,yb) in enumerate(dl):\n",
    "            latent = self.enc(xb)\n",
    "            c_pred = self.lin1(latent[:,:,0,0])\n",
    "            r_pred = self.dec(latent)\n",
    "            mask = yb>=0\n",
    "            yb_ = yb*mask\n",
    "            c_loss_ = self.c_loss_func(c_pred,yb_)\n",
    "            print(c_loss_)\n",
    "            c_loss = (c_loss_[mask]).mean()\n",
    "            r_loss = self.loss_func(r_pred,xb)\n",
    "            loss = r_loss.mean() + c_loss\n",
    "            print(loss)\n",
    "            loss.backward(); self.opt.step(); self.opt.zero_grad()\n",
    "            \n",
    "    \n",
    "    def check_ae_images(self,num_rows=5):\n",
    "        idxs = np.random.randint(0,len(self.dataset),size=num_rows*4)\n",
    "        inimgs = self.dataset[idxs]\n",
    "        x = self.enc(inimgs)\n",
    "#         x = self.lin(x[:,:,0,0])[:,:,None,None]\n",
    "        outimgs = self.dec(x)\n",
    "        _, axes = plt.subplots(num_rows,4,figsize=(7,7))\n",
    "        for i in range(num_rows):\n",
    "            axes[i,0].imshow(inimgs[i,0])\n",
    "            axes[i,1].imshow(outimgs[i,0])\n",
    "            axes[i,2].imshow(inimgs[i+num_rows,0])\n",
    "            axes[i,3].imshow(outimgs[i+num_rows,0])\n",
    "\n",
    "    def check_concept_images(self,num_imgs=5):\n",
    "        outimgs = self.dec(self.concepts.exemplars_as_tensor[:25,:,None,None]).squeeze(1)\n",
    "        num_imgs = min(len(outimgs),25)\n",
    "        _, axes = plt.subplots(5,5,figsize=(7,7))\n",
    "        for i in range(num_imgs):\n",
    "            axes.flatten()[i].imshow(outimgs[i])\n",
    "    def vis_latent(self,l): plt.imshow(self.dec(l[None,:,None,None].cuda())[0,0]); plt.show()\n",
    "    def vis_ae(self,x): vis_latent(self.enc(x.cuda()).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ones = mnist_train.x[(mnist_train.y == 1)].to('cuda')\n",
    "twos = mnist_train.x[(mnist_train.y == 2)].to('cuda')\n",
    "threes = mnist_train.x[(mnist_train.y == 3)].to('cuda')\n",
    "fours = mnist_train.x[(mnist_train.y == 4)].to('cuda')\n",
    "fives = mnist_train.x[(mnist_train.y == 5)].to('cuda')\n",
    "sixes = mnist_train.x[(mnist_train.y == 6)].to('cuda')\n",
    "sevens = mnist_train.x[(mnist_train.y == 7)].to('cuda')\n",
    "eights = mnist_train.x[(mnist_train.y == 8)].to('cuda')\n",
    "nines = mnist_train.x[(mnist_train.y == 9)].to('cuda')\n",
    "simple_ds = utils.TransformDataset(torch.cat([ones,twos,fives,eights],dim=0), [utils.to_float_tensor,utils.add_colour_dimension],x_only=True,device='cuda')\n",
    "simple_clearner = ConceptLearner(enc,dec,lin,simple_ds,opt=torch.optim.Adam,loss_func=nn.L1Loss(reduction='none'))\n",
    "# %lprun -f simple_clearner.train_ae simple_clearner.train_ae(epochs=3,bs=64,gan_lmbda=0.01,nonsense_lmbda=1)\n",
    "# simple_clearner.train_ae(epochs=25,bs=32,gan_lmbda=0.00,nonsense_lmbda=0.00,ohe_lmbda=0)\n",
    "# simple_clearner.check_ae_images()\n",
    "simple_clearner.train_ae(epochs=10,bs=64,gan_lmbda=0.01,nonsense_lmbda=0.0,ohe_lmbda_max=0.1)\n",
    "simple_clearner.check_ae_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(enc[1][3][0].weight.shape[0]):\n",
    "    simple_clearner.vis_latent((torch.arange(enc[1][3][0].weight.shape[0])==t).float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload()\n",
    "def check(number):\n",
    "    dl = utils.get_dloader(number,x_only=True)\n",
    "    xb = next(iter(dl))\n",
    "    return enc(xb).argmax(dim=1)[:20]\n",
    "check(fives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_clearner.train_concepts(epochs=1,bs=1,gan_lmbda=0.01,eps=0.001,thresh=0.55)\n",
    "simple_clearner.check_concept_images()\n",
    "[e.age for e in simple_clearner.concepts.exemplars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = enc(to_float_tensor(add_colour_dimension(twos[:2000])))\n",
    "l1 /= l1.norm(dim=1,keepdim=True)\n",
    "l1 = l1.mean(dim=0)[:,0,0]\n",
    "l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = enc(to_float_tensor(add_colour_dimension(fours[:2000])))\n",
    "l2 /= l2.norm(dim=1,keepdim=True)\n",
    "l2=l2.mean(dim=0)\n",
    "l3 = enc(to_float_tensor(add_colour_dimension(eights[:2000])))\n",
    "l3 /= l3.norm(dim=1,keepdim=True)\n",
    "l3=l3.mean(dim=0)\n",
    "print(l1.shape,l2.shape,l3.shape)\n",
    "simple_clearner.vis_latent(l1[:,0,0])\n",
    "simple_clearner.vis_latent(l2[:,0,0])\n",
    "simple_clearner.vis_latent(l3[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_enc(): \n",
    "    plt.imshow(dec(utils.normalize(torch.randn(1,NZ,1,1).cuda())).squeeze())\n",
    "    plt.show()\n",
    "test_enc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_one_of(mod,mod_types):\n",
    "    if mod_types == 'all': return True\n",
    "    res = False\n",
    "    for mod_type in mod_types: res = res or isinstance(mod,mod_type)\n",
    "    return res\n",
    "\n",
    "def printshapes(mod,inp,outp):\n",
    "    print(f'\\nInside {mod.__class__.__name__} forward')\n",
    "    print(f'Input shape: {inp[0].shape}')\n",
    "    print(f'Output shape: {outp.data.shape}')   \n",
    "    \n",
    "def printstats(mod,inp,outp):\n",
    "    print(f'\\nInside {mod.__class__.__name__} forward')\n",
    "    print(f'Input mean: {inp[0].mean()}')\n",
    "    print(inp[0][0])\n",
    "    print(f'Output mean: {outp.data.mean()}')   \n",
    "    print(outp[0][0])\n",
    "    \n",
    "def printshapesstats(mod,inp,outp):\n",
    "    printshapes(mod,inp,outp)\n",
    "    printstats(mod,inp,outp)\n",
    "    \n",
    "def hook_layers_of_types(model,hook,layertypes):\n",
    "    hooks = []\n",
    "    for child in model.children():\n",
    "        if is_one_of(child,layertypes): hooks.append(child.register_forward_hook(hook))\n",
    "        hooks += hook_layers_of_types(child,hook,layertypes)\n",
    "    return hooks\n",
    "\n",
    "def test_model(model,test_batch,test_func,layertypes):\n",
    "    added_hooks = hook_layers_of_types(model,test_func,layertypes)\n",
    "    try: model(test_batch)\n",
    "    except Exception as e: print(e)\n",
    "    finally:\n",
    "        for hook in added_hooks: hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc,dec = utils.get_enc_dec(device='cuda', latent_size=NZ)\n",
    "# enc,dec,mnist_dataset.data = enc.to('cuda'),dec.to('cuda'),mnist_dataset.data.to('cuda')\n",
    "dl = utils.get_dloader(ones,x_only=True)\n",
    "test_batch = next(iter(dl))\n",
    "test_model(enc,test_batch,printshapes,'all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
