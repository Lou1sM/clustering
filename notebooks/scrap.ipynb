{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exemplar():\n",
    "    def __init__(self,tensor): self.tensor,self.count=tensor,0\n",
    "        \n",
    "def add_exemplar(new):\n",
    "    new_exemplar = Exemplar(new)\n",
    "    exemplars.append(new_exemplar)\n",
    "    return new_exemplar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "def check_thresh_maybe_replace(target_batch,dist_idx):\n",
    "    dist,idx = dist_idx[0], dist_idx[1]\n",
    "    if dist < thresh: return exemplars[idx].tensor\n",
    "    else: return add_exemplar(target_batch[int(idx)].data.requires_grad_(True)).tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_batch = torch.randn(64,10)\n",
    "dummy_exemplars = torch.randn(29,10)\n",
    "exemplars = [Exemplar(e) for e in dummy_exemplars]\n",
    "distances = (dummy_batch[:,None] - dummy_exemplars[None]).norm(dim=-1)\n",
    "m,a = torch.max(distances,dim=-1)\n",
    "#firsts = list(zip(*torch.max(distances,dim=-1)))\n",
    "t=torch.stack(list(map(partial(check_thresh_maybe_replace,dummy_batch),zip(m,a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.7775, 0.0000, 0.0000, 5.6696, 0.0000, 5.9632, 5.6762, 5.2963, 5.6225,\n",
       "        5.7052, 5.8223, 5.3899, 5.9176, 0.0000, 5.5882, 0.0000, 5.5566, 0.0000,\n",
       "        0.0000, 5.8893, 0.0000, 5.6927, 5.9053, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 5.7672, 0.0000, 0.0000, 5.5124, 5.9814,\n",
       "        5.6485, 5.2750, 0.0000, 0.0000, 0.0000, 0.0000, 5.1013, 5.4682, 5.7247,\n",
       "        5.4266, 5.8531, 5.7059, 0.0000, 5.8913, 4.7145, 0.0000, 0.0000, 5.4896,\n",
       "        5.6029, 0.0000, 0.0000, 0.0000, 5.5553, 0.0000, 0.0000, 5.2330, 0.0000,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m*(m<6).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64]), torch.Size([64]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(exemplars):\n",
    "    if [int(f[1]) for f in firsts].count(i) <= 0: exemplars.remove(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(8,3)\n",
    "b = torch.rand(8,10)@torch.randn(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(1.1409), tensor(2)),\n",
       " (tensor(0.8191), tensor(2)),\n",
       " (tensor(0.6731), tensor(0)),\n",
       " (tensor(0.8257), tensor(0)),\n",
       " (tensor(0.5204), tensor(2)),\n",
       " (tensor(1.3387), tensor(0)),\n",
       " (tensor(0.5739), tensor(1)),\n",
       " (tensor(0.8581), tensor(2))]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*a.max(dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(-0.7947), tensor(1)),\n",
       " (tensor(1.0621), tensor(2)),\n",
       " (tensor(1.0095), tensor(1)),\n",
       " (tensor(1.8837), tensor(1)),\n",
       " (tensor(1.8896), tensor(1)),\n",
       " (tensor(-1.7801), tensor(2)),\n",
       " (tensor(-0.3573), tensor(1)),\n",
       " (tensor(1.0881), tensor(2))]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*b.max(dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5127,  0.2142,  0.0175],\n",
       "        [-1.0178, -0.2909, -0.4876],\n",
       "        [-0.4900,  0.2369,  0.0402],\n",
       "        [-0.6114,  0.1155, -0.0812]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4,7)\n",
    "b = torch.randn(3,7)\n",
    "(a[:,None]-b[None]).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.2880, 2.0825, 1.3543, 1.4101, 1.5826, 2.8698, 0.7836, 2.6263, 0.5303,\n",
       "        1.5768, 1.0591, 1.6703, 2.3666, 2.9965, 1.3401, 1.7124, 2.7773, 0.1335,\n",
       "        1.4649, 1.4456, 2.0718, 2.2106, 1.8924, 3.1854, 1.7584, 1.4617, 1.3676,\n",
       "        2.5433, 0.7217, 0.9879, 1.1285, 2.1203, 1.6035, 1.3906, 2.2708, 4.0045,\n",
       "        1.6361, 1.6168, 1.2225, 0.8700, 1.1566, 2.1260, 2.8821, 1.5173, 2.5218,\n",
       "        0.8986, 3.2847, 1.8594, 1.5792, 0.7418, 1.7054, 1.4648, 2.0340, 1.9783,\n",
       "        1.5884, 2.7752, 1.8774, 1.3479, 1.7480, 1.4729, 2.0008, 2.0591, 2.2606,\n",
       "        1.8627]),\n",
       "indices=tensor([8, 5, 4, 1, 9, 3, 7, 4, 3, 1, 2, 5, 1, 5, 2, 7, 3, 0, 8, 7, 4, 0, 5, 4,\n",
       "        0, 2, 8, 6, 7, 5, 8, 1, 4, 7, 6, 7, 4, 0, 7, 8, 7, 9, 8, 2, 0, 4, 0, 8,\n",
       "        5, 8, 2, 0, 1, 9, 3, 0, 3, 2, 3, 9, 2, 9, 0, 8]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_enc_dec_mu_var():\n",
    "    block1 = get_reslike_block([1,4,8,16,32],sz=7)\n",
    "    block2 = get_reslike_block([32,64,128,256],sz=1)\n",
    "    mu_net = nn.Linear(256,NZ)\n",
    "    var_net = nn.Linear(256,NZ)\n",
    "    enc = nn.Sequential(block1,block2)\n",
    "    dec = Generator(nz=NZ,ngf=32,nc=1)    \n",
    "    return enc,dec,mu_net,var_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE():\n",
    "    def __init__(self,enc,dec,prior,dataset,opt,loss_func): self.enc,self.dec,self.prior,self.opt,self.loss_func = enc,dec,prior,opt,loss_func\n",
    "    def one_batch(self,xb):\n",
    "        latent = self.enc(xb)\n",
    "        klloss = kl_divergence(latent,self.prior)\n",
    "        pred = self.dec(latent)\n",
    "        rloss = self.loss_func(preds,xb)\n",
    "        self.writer('rloss',rloss,self.batch_idx)\n",
    "        self.writer('klloss',klloss,self.batch_idx)\n",
    "        loss = rloss*self.lmbda*klloss\n",
    "        loss.backward(); self.opt.step(); self.opt.zero_grad()\n",
    "        self.batch_idx += 1\n",
    "    def train(self,epochs,bs):\n",
    "        dl = data.DataLoader(self.dataset,batch_sampler=data.BatchSampler(data.RandomSampler(mnist_dataset),bs,drop_last=True))        \n",
    "        for epoch in range(epochs):\n",
    "            self.batch_idx = 0\n",
    "            for xb in dl: one_batch(xb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(mu1,logvar1,mu2,logvar2):\n",
    "    kl = -0.5*torch.sum(1+logvar1-logvar2 - (logvar1.exp() + (mu1-mu2).pow(2)) / logvar2.exp())\n",
    "    try: assert kl >= 0\n",
    "    except: set_trace()\n",
    "    return kl\n",
    "def kl_to_zero_one(mu,logvar):\n",
    "    return -0.5 * torch.sum((1+logvar-logvar.exp()-mu.pow(2)))\n",
    "def pseudo_sample(mu,logvar): return mu + torch.randn(logvar.shape)*logvar.exp()\n",
    "a=pseudo_sample(torch.tensor([0.,0.,0.]), torch.tensor([1.,1.5,2.]))\n",
    "assert (a[torch.max(a,0)[1]] == torch.max(a,0)[0]).all()\n",
    "a,torch.max(a,0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-dfbdbbd5e0d5>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-dfbdbbd5e0d5>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    def __init__(self,enc,dec,mu_net,var_net,dataset,opt,loss_func):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    def train_concepts_dummy(self,epochs,eps=0.5):\n",
    "        t1,t2 = normalize(torch.randn(10, device='cuda')), normalize(torch.randn(10, device='cuda'))\n",
    "        t3,t4 = -t1,-t2\n",
    "        assert(t1.norm()==t2.norm()==t3.norm()==t4.norm()==1)\n",
    "        for i in range(50000):\n",
    "            new = random.choice([t1,t2,t3,t4]) + 0.01*torch.randn(10,device='cuda')\n",
    "            self.concepts.interpret_wo_grad(new,eps)\n",
    "            if i%1000==0: print(i, len(self.concepts), self.concepts.thresh)\n",
    "        \n",
    "    def interpret(self,inp_):\n",
    "        assert sum([e.tensor.requires_grad for e in self.exemplars]) == len(self)\n",
    "        for e in self.exemplars:\n",
    "            e.count+=1\n",
    "            e.age+=1\n",
    "            if e.count == 20: \n",
    "                self.exemplars.remove(e)\n",
    "                try: self.opt.param_groups[-1]['params'] = [t for t in self.opt.param_groups[-1]['params'] if not (t==e.tensor).all()]\n",
    "                except: set_trace()\n",
    "        #dist,idx = torch.max(torch.tensor([self.dist_func(inp_,e.tensor) for e in self.exemplars]),0)\n",
    "        try: dist,idx = torch.min((self.exemplars_as_tensor-inp_).norm(dim=-1),0)\n",
    "        except RuntimeError: pass # No exemplars defined yet\n",
    "        if dist > self.thresh:\n",
    "            self.add_exemplar(inp_.data.clone().squeeze(0).requires_grad_(True))\n",
    "            loss = torch.tensor(0.,device='cuda')\n",
    "        else:\n",
    "            self.exemplars[idx].count=0\n",
    "            loss = dist\n",
    "        if len(self) > 10: self.thresh *= 1.05\n",
    "        elif len(self) < 10 and self.thresh < 10: self.thresh *= 0.95\n",
    "        assert sum([e.tensor.requires_grad for e in self.exemplars]) == len(self)\n",
    "        return loss\n",
    "    \n",
    "class VConceptLearner():\n",
    "    def __init__(self,enc,dec,mu_net,var_net,dataset,opt,loss_func): \n",
    "        self.enc,self.dec,self.mu_net,self.var_net,self.dataset,self.loss_func = enc,dec,mu_net,var_net,dataset,loss_func\n",
    "        self.opt = opt(params = [{'params':enc.parameters()},{'params':dec.parameters()}])\n",
    "        self.writer = SummaryWriter()\n",
    "        self.init_concepts()\n",
    "        \n",
    "    def init_concepts(self): self.concepts = Concepts(NZ,opt=self.opt,thresh=1.0,eps=0.3,num_starting_concepts=2)\n",
    "    def test_shapes(self): test_shapes(self.ae)\n",
    "    def show_one_concept(self,idx=0): \n",
    "        plt.imshow(self.dec((self.concepts.exemplars[idx].tensor)[None,:,None,None]).squeeze())\n",
    "        plt.show()\n",
    "        \n",
    "    def train_ae(self,epochs,bs,lmbda):\n",
    "        dl = data.DataLoader(self.dataset,batch_sampler=data.BatchSampler(data.RandomSampler(mnist_dataset),bs,drop_last=True))        \n",
    "        for param in self.enc.parameters(): param.requires_grad = True\n",
    "        for param in self.dec.parameters(): param.requires_grad = True    \n",
    "        for epoch in range(epochs):\n",
    "            for i, xb in enumerate(dl): \n",
    "                latent = self.enc(xb).squeeze()\n",
    "                mu,logvar = self.mu_net(latent), self.var_net(latent)\n",
    "                kl_loss = kl_to_zero_one(mu,logvar)\n",
    "                sample = pseudo_sample(mu,logvar)\n",
    "                pred = self.dec(sample[:,:,None,None])\n",
    "                rloss = self.loss_func(xb,pred)\n",
    "                loss = rloss + lmbda*kl_loss\n",
    "                loss.backward(); \n",
    "                self.tensorboard_model_params()\n",
    "                self.opt.step(); self.opt.zero_grad()\n",
    "                self.writer.add_scalar('ae/rloss',loss,i)\n",
    "            self.writer.add_images('Test_batch', pred,epoch)\n",
    "            \n",
    "    def tensorboard_model_params(self):\n",
    "        for i,layer in enumerate(self.enc.modules()): \n",
    "            try: \n",
    "                grads = [p.grad.mean() for p in layer.parameters()]\n",
    "                self.writer.add_histogram(f'enc_grad/{i}',sum(grads)/len(grads),i)\n",
    "            except (AttributeError,ZeroDivisionError): pass\n",
    "        for i,layer in enumerate(self.dec.modules()): \n",
    "            try: self.writer.add_histogram(f'dec_grad/{i}',sum(grads)/len(grads),i)\n",
    "            except (AttributeError,ZeroDivisionError): pass        \n",
    "            \n",
    "    def tensorboard_model_params(self):\n",
    "        for i,layer in enumerate(self.enc.modules()): \n",
    "            try: \n",
    "                grads = [p.grad.mean() for p in layer.parameters()]\n",
    "                self.writer.add_histogram(f'enc_grad/{i}',sum(grads)/len(grads),i)\n",
    "            except (AttributeError,ZeroDivisionError): pass\n",
    "        for i,layer in enumerate(self.dec.modules()): \n",
    "            try: self.writer.add_histogram(f'dec_grad/{i}',sum(grads)/len(grads),i)\n",
    "            except (AttributeError,ZeroDivisionError): pass        \n",
    "            \n",
    "    \n",
    "    def train_concepts(self,epochs,bs,lmbda):\n",
    "        batch = (bs!=1)\n",
    "        dl = data.DataLoader(self.dataset,batch_sampler=data.BatchSampler(data.RandomSampler(mnist_dataset),bs,drop_last=True))\n",
    "        for param in self.enc.parameters(): param.requires_grad = False\n",
    "        for param in self.dec.parameters(): param.requires_grad = False    \n",
    "        for epoch in range(epochs):\n",
    "            for i, xb in enumerate(dl): \n",
    "                latent = self.enc(xb)[:,:,0,0]\n",
    "                mu,logvar = self.mu_net(latent), self.var_net(latent)       \n",
    "                sample = pseudo_sample(mu,logvar)\n",
    "                pred = self.dec(sample[:,:,None,None])\n",
    "                rloss = self.loss_func(xb,pred)\n",
    "                lloss = self.concepts.interpret_batch(mu,logvar) if batch else self.concepts.interpret(mu,logvar)\n",
    "                loss = rloss + lmbda*lloss\n",
    "                loss.backward();\n",
    "                self.tensorboard_model_params()\n",
    "                self.opt.step(); self.opt.zero_grad()\n",
    "                if i%10000==0: \n",
    "                    self.writer.add_images('images_of_concepts',pred)\n",
    "            print(f'{i} batches')\n",
    "    \n",
    "    def check_ae_images(self,num_rows=5):\n",
    "        inimgs = self.dataset[:num_rows*4]\n",
    "        x = self.enc(inimgs).squeeze()\n",
    "        x = self.mu_net(x)[:,:,None,None]\n",
    "        outimgs = self.dec(x)\n",
    "        _, axes = plt.subplots(num_rows,4,figsize=(7,7))\n",
    "        for i in range(num_rows):\n",
    "            axes[i,0].imshow(inimgs[i,0])\n",
    "            axes[i,1].imshow(outimgs[i,0])\n",
    "            axes[i,2].imshow(inimgs[i+num_rows,0])\n",
    "            axes[i,3].imshow(outimgs[i+num_rows,0])\n",
    "\n",
    "    def check_concept_images(self,num_imgs=5):\n",
    "        outimgs = self.dec(self.concepts.exemplars_as_tensor[:25,:,None,None]).squeeze(1)\n",
    "        num_imgs = min(len(outimgs),25)\n",
    "        _, axes = plt.subplots(5,5,figsize=(7,7))\n",
    "        for i in range(num_imgs):\n",
    "            axes.flatten()[i].imshow(outimgs[i])\n",
    "            \n",
    "class VConcepts():\n",
    "    def __init__(self,dim,opt,thresh,eps,num_starting_concepts=10): \n",
    "        self.dim,self.opt,self.thresh,self.eps=dim,opt,thresh,eps\n",
    "        v1,v2 = torch.randn(self.dim),torch.rand(self.dim)\n",
    "        self.exemplars = []; params = []\n",
    "        for _ in range(num_starting_concepts):\n",
    "            mu = torch.randn(self.dim).requires_grad_(True)\n",
    "            sig = torch.randn(self.dim).requires_grad_(True)\n",
    "            self.exemplars.append(Exemplar(mu,sig))\n",
    "            params.append(mu); params.append(sig)\n",
    "        assert len(self) == num_starting_concepts\n",
    "        self.opt.add_param_group({'params': params, 'lr':1e-5})\n",
    "        \n",
    "    def add_exemplar(self,mu_tensor,logvar_tensor):\n",
    "        new_exemplar = Exemplar(mu_tensor,logvar_tensor)\n",
    "        self.exemplars.append(new_exemplar)\n",
    "        self.opt.param_groups[-1]['params'] += [mu_tensor,logvar_tensor]\n",
    "        return new_exemplar\n",
    " \n",
    "    @property\n",
    "    def exemplars_as_tensor(self): return torch.stack([e.mu_tensor for e in self.exemplars])\n",
    "    def __len__(self): return len(self.exemplars)\n",
    "        \n",
    "    def check_thresh_maybe_replace(self,target_batch,dist_idx_latent):\n",
    "        dist,idx,inp_latent_vec = dist_idx_latent[0], dist_idx_latent[1], dist_idx_latent[2]\n",
    "        assert 0<= idx and idx < len(self) and dist >= 0\n",
    "        if dist < self.thresh: return self.exemplars[idx].tensor\n",
    "        else: return self.add_exemplar(inp_latent_vec.data.requires_grad_(True)).tensor\n",
    "\n",
    "    def interpret_batch(self,inp_):\n",
    "        #for e in self.exemplars: e.count += 1\n",
    "        assert sum([e.tensor.requires_grad for e in self.exemplars]) == len(self)\n",
    "        inp = inp_.data.view(inp_.shape[0],-1)\n",
    "        distances = (inp[:,None] - self.exemplars_as_tensor[None]).norm(dim=-1)\n",
    "        maxes,winning_idxs = torch.max(distances,dim=-1)\n",
    "        winning_idxs = list(winning_idxs)\n",
    "        to_remove=[]\n",
    "        #for i,e in enumerate(self.exemplars):\n",
    "        #    if winning_idxs.count(i) == 0: to_remove.append(e)\n",
    "        firsts=torch.stack(list(map(partial(self.check_thresh_maybe_replace,inp),zip(maxes,winning_idxs,inp))))\n",
    "        mask = maxes<self.thresh\n",
    "        self.exemplars = [e for e in self.exemplars if e not in to_remove]\n",
    "        #print(len(to_remove))\n",
    "        assert sum([e.tensor.requires_grad for e in self.exemplars]) == len(self)\n",
    "        main_loss = (maxes*(maxes<self.thresh).float()).mean()\n",
    "        close_loss = (firsts[:,None]-self.exemplars_as_tensor[None]).norm(dim=-1).mean()\n",
    "        return firsts.reshape(inp_.shape),main_loss, close_loss\n",
    "        \n",
    "    def interpret(self,mu,logvar):\n",
    "        assert sum([e.mu.requires_grad for e in self.exemplars]) == len(self)\n",
    "        for e in self.exemplars:\n",
    "            e.count+=1\n",
    "            if e.count == 20: self.exemplars.remove(e)\n",
    "        kl_dist,idx = torch.max(torch.tensor([kl(v.mu,v.logvar,mu,logvar) for v in self.exemplars]),0)    \n",
    "        if kl_dist > self.thresh:\n",
    "            new_mu = mu.data.requires_grad_(True)\n",
    "            new_logvar = logvar.data.requires_grad_(True)\n",
    "            first = self.add_exemplar(new_mu,new_logvar)\n",
    "            assert torch.allclose(kl(first.mu,first.logvar,mu,logvar),torch.tensor(0.),atol=1e-5)\n",
    "            loss = 0\n",
    "        else:\n",
    "            self.exemplars[idx].count=0\n",
    "            loss = kl_dist\n",
    "        if len(self) > 10: self.thresh *= 1.05\n",
    "        elif len(self) < 10: self.thresh *= 0.95\n",
    "        return loss\n",
    "    \n",
    "        def train_concepts(self,epochs,bs,gan_lmbda,eps,thresh):\n",
    "        since_update = 0\n",
    "        self.concepts.thresh = thresh\n",
    "        dl = data.DataLoader(self.dataset,batch_sampler=data.BatchSampler(data.RandomSampler(self.dataset),bs,drop_last=True),pin_memory=False)\n",
    "        for param in self.enc.parameters(): param.requires_grad = False\n",
    "        for param in self.dec.parameters(): param.requires_grad = True \n",
    "        self.opt.param_groups[1]['lr'] = 1e-6\n",
    "        for epoch in range(epochs):\n",
    "            for i, xb in enumerate(dl): \n",
    "                start_time = time()\n",
    "                latent = self.enc(xb)[:,:,0,0]\n",
    "                latent = latent/latent.norm(dim=-1,keepdim=True)\n",
    "                best_match, dist = self.concepts.interpret_wo_grad(latent,eps)\n",
    "                if dist == 0: since_update = 0; self.concepts.thresh*=1.01\n",
    "                else: since_update + 1\n",
    "                if since_update%100 == 0 and since_update > 0: self.concepts.thresh *= 0.95\n",
    "                pred = self.dec(best_match.tensor[None,:,None,None])\n",
    "                real_guess, fake_guess, noise_guess = self.discrim(xb), self.discrim(pred), self.discrim(torch.rand_like(xb,device='cuda'))\n",
    "                dfakeloss,dnoiseloss,drealloss = -(fake_guess+1e-3).log().mean(), - (1-noise_guess+1e-3).log().mean(), - (real_guess+1e-3).log().mean()\n",
    "                discrim_loss = dfakeloss+dnoiseloss+drealloss\n",
    "                pixel_loss,gan_loss = self.loss_func(xb,pred), (fake_guess+1e-3).log().mean()\n",
    "                if pixel_loss > 0.2: self.concepts.add_exemplar(latent.data.clone().squeeze(0))\n",
    "                loss = pixel_loss + gan_lmbda*gan_loss\n",
    "                if i%1000==0: print_tensors(loss,dfakeloss,dnoiseloss,drealloss)\n",
    "                loss.backward(retain_graph=True); \n",
    "                self.opt.step(); self.opt.zero_grad(); self.discrim_opt.zero_grad() \n",
    "                if discrim_loss<loss: continue\n",
    "                if i > 5000: break\n",
    "            print(f'{i} batches')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [torch.randn(3).requires_grad_(True) for _ in range(5)]\n",
    "tensors = torch.stack(tensors)\n",
    "b = torch.randn(3).requires_grad_(True)\n",
    "dists = (tensors-b).norm(dim=-1)\n",
    "l, _ = torch.max(dists,0)\n",
    "dists.requires_grad, l.requires_grad\n",
    "\n",
    "tensors = [torch.randn(3).requires_grad_(True) for _ in range(5)]\n",
    "print([t.requires_grad for t in tensors])\n",
    "b = torch.randn(3).requires_grad_(True)\n",
    "dists = torch.tensor([(a-b).norm() for a in tensors])\n",
    "dists.requires_grad\n",
    "\n",
    "l,_=torch.max(torch.tensor([(a-b).norm() for a in tensors]),0)\n",
    "#l.requires_grad = True\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = torch.load('10d.pt')\n",
    "#enc,dec = l['enc'], l['dec']\n",
    "#enc,dec=get_enc_dec()\n",
    "#enc,dec,mnist_dataset.data = enc.to('cuda'),dec.to('cuda'),mnist_dataset.data.to('cuda')\n",
    "#clearner = ConceptLearner(enc,dec,mnist_dataset,opt=torch.optim.Adam,loss_func=nn.L1Loss())\n",
    "#%time clearner.train_ae(epochs=3,bs=64)\n",
    "#clearner.check_ae_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 real_guess, fake_guess, noise_guess = self.discrim(xb), self.discrim(pred), self.discrim(torch.rand_like(xb,device='cuda'))\n",
    "#                 dfakeloss,dnoiseloss,drealloss = -(fake_guess+1e-3).log().mean(), - (1-noise_guess+1e-3).log().mean(), - (real_guess+1e-3).log().mean()\n",
    "#                 discrim_loss = dfakeloss+dnoiseloss+drealloss\n",
    "#                 if i > 100:\n",
    "#                     random_latent = utils.get_far_tensor(latent)\n",
    "#                     should_be_nonsense = self.dec(random_latent[None])\n",
    "#                     nonsense_score = self.discrim(should_be_nonsense)\n",
    "#                 else: nonsense_score = torch.tensor(0.,device='cuda')\n",
    "#                 loss_ = self.loss_func(xb,pred) + gan_lmbda*(fake_guess+1e-3).log().mean() - nonsense_lmbda*(nonsense_score+1e-3).log().mean()\n",
    "#                 if discrim_loss<loss: continue\n",
    "#                 discrim_loss.backward(); self.discrim_opt.step(); self.discrim_opt.zero_grad(); self.opt.zero_grad()        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
